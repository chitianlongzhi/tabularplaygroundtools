# !(if [ -d tabularplaygroundtools ]; then rm -rf tabularplaygroundtools ; fi)
# !git clone https://github.com/chitianlongzhi/tabularplaygroundtools.git
# !cat tabularplaygroundtools/tabularplaygroundtools.py
# import tabularplaygroundtools.tabularplaygroundtools
# param = {
#     'train.file': '/kaggle/input/playground-series-s5e3/train.csv',
#     'test.file': '/kaggle/input/playground-series-s5e3/test.csv',
#     'CorrelationAnalysis': True,
#     'target': 'rainfall',
#     'type': 'LSTM'
# }
# tabularplaygroundtools.tabularplaygroundtools.tabularplaygroundtools(param)
import pandas as pd
class tabularplaygroundtools:
  def __init__(self, param):
    if not 'type' in param:
      print('ERROR: type')
      return
    if not 'train.file' in param:
      print('ERROR: train.file')
      return
    if not 'test.file' in param:
      print('ERROR: test.file')
      return
    if not 'target' in param:
      print('ERROR: target')
      return
    print('# Loading Data')
    print('import pandas as pd')
    print('train = pd.read_csv(\'{}\')'.format(param['train.file']))
    train = pd.read_csv(param['train.file'])
    print('test = pd.read_csv(\'{}\')'.format(param['test.file']))
    test = pd.read_csv(param['test.file'])
    print('# Missing Values')
    if 'SimpleImputer' in param:
      print('from sklearn.impute import SimpleImputer')
      print('imputer = SimpleImputer(strategy=\'mean\')')
      print('print(train.isnull().sum())')
      for c, v in train.isnull().sum().items():
        if v>0:
          print('train[[\'{}\']] = imputer.fit_transform(train[[\'{}\']])'.format(c, c))
      print('print(test.isnull().sum())')
      for c, v in test.isnull().sum().items():
        if v>0:
          print('test[[\'{}\']] = imputer.fit_transform(test[[\'{}\']])'.format(c, c))
    else:
      print('print(train.isnull().sum())')
      for c, v in train.isnull().sum().items():
        if v>0:
          print('train[\'{}\'] = train[\'{}\'].fillna(train[\'{}\'].median())'.format(c, c, c))
      print('print(test.isnull().sum())')
      for c, v in test.isnull().sum().items():
        if v>0:
          print('test[\'{}\'] = test[\'{}\'].fillna(test[\'{}\'].median())'.format(c, c, c))
    if 'CorrelationAnalysis' in param:
      print('# Correlation Analysis')
      print('import matplotlib.pyplot as plt')
      print('import seaborn as sns')
      print('numerical_features = train.select_dtypes(include=[\'int64\', \'float64\']).columns.tolist()')
      print('corr = train[numerical_features].corr()')
      print('plt.figure(figsize=(12, 10))')
      print('sns.heatmap(corr, annot=True, cmap=\'coolwarm\')')
      print('plt.title(\'Correlation Heatmap\')')
      print('plt.show()')
    print('# Split data')
    print('X = train.drop([\'id\', \'{}\'], axis=1)'.format(param['target']))
    print('y = train[\'{}\']'.format(param['target']))
    print('from sklearn.model_selection import train_test_split')
    print('X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=1)')
    print('X_test = test.drop([\'id\',], axis=1)')
    print('# Scaling')
    if 'RobustScaler' in param:
      print('from sklearn.preprocessing import RobustScaler')
      print('scaler = RobustScaler()')
    else:
      print('from sklearn.preprocessing import StandardScaler')
      print('scaler = StandardScaler()')
    print('X_train = scaler.fit_transform(X_train)')
    print('X_valid = scaler.transform(X_valid)')
    print('X_test = scaler.transform(X_test)')
    if param['type'] == 'LSTM':
      print('# reshape for LSTM')
      print('X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))')
      print('X_valid = X_valid.reshape((X_valid.shape[0], 1, X_valid.shape[1]))')
      print('# LSTM model')
      print('from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LeakyReLU, GRU')
      print('inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))')
      print('x = LSTM(256, activation=\'tanh\', return_sequences=True)(inputs)')
      print('x = Dropout(0.3)(x)')
      print('x = LSTM(128, activation=\'tanh\', return_sequences=True)(x)')
      print('x = Dropout(0.3)(x)')
      print('x = LSTM(64, activation=\'tanh\', return_sequences=True)(x)')
      print('x = Dropout(0.3)(x)')
      print('x = LSTM(32, activation=\'tanh\')(x)')
      print('x = Dense(16, activation=\'tanh\')(x)')
      print('outputs = Dense(1, activation=\'sigmoid\')(x)')
      print('import tensorflow as tf')
      print('learning_rate = 0.001')
      print('model = tf.keras.Model(inputs=inputs, outputs=outputs)')
      print('model.compile(')
      print('    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),')
      print('    loss=\'binary_crossentropy\',')
      print('    metrics=[')
      print('        \'accuracy\',')
      print('        tf.keras.metrics.AUC(name=\'auc\'),')
      print('        tf.keras.metrics.Precision(name=\'precision\'),')
      print('        tf.keras.metrics.Recall(name=\'recall\')')
      print('    ]')
      print(')')
      print('model.summary()')
      print('# Define callbacks')
      print('patience = 20')
      print('early_stopping = tf.keras.callbacks.EarlyStopping(')
      print('    monitor=\'val_loss\',')
      print('    patience=patience,')
      print('    restore_best_weights=True,')
      print('    verbose=1')
      print(')')
      print('reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(')
      print('    monitor=\'val_loss\',')
      print('    factor=0.2,')
      print('    patience=patience//2,')
      print('    min_lr=1e-6,')
      print('      verbose=1')
      print(')')
      print('%%time')
      print('# fitting')
      print('epochs = 100')
      print('batch_size = 32')
      print('history = model.fit(')
      print('    X_train, y_train,')
      print('    validation_data=(X_valid, y_valid),')
      print('    epochs=epochs,')
      print('    batch_size=batch_size,')
      print('    callbacks=[early_stopping, reduce_lr],')
      print('    verbose=1')
      print(')')
      print('%%time')
      print('# predicting')
      print('X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))')
      print('test[\'{}\'] = model.predict(X_test)'.format(param['target']))
    if param['type'] == 'Tensorflow':
      print('import warnings')
      print('warnings.filterwarnings(\'ignore\')')
      print('import tensorflow as tf')
      print('print(tf.test.is_gpu_available())')
      print('print(tf.test.is_built_with_cuda())')
      print('# Tensorflow model')
      print('model = tf.keras.models.Sequential()')
      print('model.add(tf.keras.layers.Dense(128, activation=\'relu\', use_bias=True, input_shape=(X_train.shape[1],)))')
      print('model.add(tf.keras.layers.Flatten())')
      print('model.add(tf.keras.layers.Dropout(0.25))')
      print('model.add(tf.keras.layers.BatchNormalization())')
      print('model.add(tf.keras.layers.Dense(units=1, activation=\'sigmoid\'))')
      print('auc = tf.keras.metrics.AUC(name=\'aucroc\')')
      print('optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00001, rho=0.9, epsilon=1e-08, decay=0.0)')
      print('model.compile(loss=\'binary_crossentropy\', optimizer=optimizer, metrics=[\'accuracy\', auc])')
      print('model.summary()')
      print('tf.keras.utils.plot_model(model, show_shapes=True)')
      print('# Define callbacks')
      print('earlystopping = tf.keras.callbacks.EarlyStopping(monitor=\'val_loss\', min_delta=0, patience=5, verbose=1, restore_best_weights=True)')
      print('reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\'val_loss\', factor=0.3, patience=3, verbose=1, min_delta=1e-4)')
      print('callbacks = [earlystopping, reduce_lr]')
      print('%%time')
      print('# fitting')
      print('history = model.fit(')
      print('    x=X_train, y=y_train,')
      print('    batch_size=128,')
      print('    shuffle=True,')
      print('    epochs=10,')
      print('    validation_data=(X_valid, y_valid),')
      print('    callbacks=callbacks')
      print(')')
      print('%%time')
      print('# predicting')
      print('test[\'{}\'] = model.predict(X_test)'.format(param['target']))
    if param['type'] == 'KFold.XGBoost':
      print('# XGBoost model')
      print('from xgboost import XGBClassifier')
      #print('import xgboost')
      print('model = XGBClassifier(')
      print('    device=\'cpu\',')
      print('    max_depth=6,')
      print('    colsample_bytree=0.9,')
      print('    subsample=0.9,')
      print('    n_estimators=10_000,')
      print('    learning_rate=0.1,')
      print('    eval_metric=\'auc\',')
      print('    early_stopping_rounds=100,')
      print('    alpha=1,')
      print(')')
      print('%%time')
      print('# fitting and predicting in KFold')
      print('from sklearn.model_selection import KFold')
      print('FOLDS = 5')
      print('kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)')
      print('oof = np.zeros(len(train))')
      print('pred = np.zeros(len(test))')
      print('for i, (train_index, valid_index) in enumerate(kf.split(train)):')
      print('    X_train = X.loc[train_index,:].copy()')
      print('    y_train = y.loc[train_index]')
      print('    X_valid = X.loc[valid_index,:].copy()')
      print('    y_valid = y.loc[valid_index]')
      print('    model.fit(')
      print('        X_train, y_train,')
      print('        eval_set=[(X_valid, y_valid)],')
      print('        verbose=100')
      print('    )')
      print('    oof[valid_index] = model.predict_proba(X_valid)[:,1]')
      print('    pred += model.predict_proba(X_test)[:,1]')
      print('from sklearn.metrics import roc_auc_score')
      print('m = roc_auc_score(train[\'{}\'].values, oof)'.format(param['target']))
      print('print(f\'XGBoost CV Score AUC = {m:.3f}\')')
      print('test[\'{}\'] = pred / FOLDS'.format(param['target']))
    print('submission = test[[\'id\', \'{}\']]'.format(param['target']))
    print('submission.to_csv(\'submission.csv\', index=False)')
